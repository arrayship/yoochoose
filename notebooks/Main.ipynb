{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from ignite.metrics import TopKCategoricalAccuracy, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 ** 13\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt_ = torch.optim.Adam\n",
    "lr = 0.00003\n",
    "val_metrics = {\n",
    "        'top-10 acc': TopKCategoricalAccuracy(10),\n",
    "        'loss': Loss(loss_fn)\n",
    "        }\n",
    "device = 'cuda:1'\n",
    "max_epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks = pd.read_csv('../data/raw/yoochoose-clicks.dat',\n",
    "        names=['sess', 'ts', 'item', 'cat'],  dtype={'cat': str},\n",
    "        usecols=['sess', 'ts', 'item'], header=None)\n",
    "clicks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt, timedelta as td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks['ts'] = clicks['ts'].apply(lambda s: dt.strptime(s[:19], '%Y-%m-%dT%H:%M:%S'))\n",
    "clicks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valsplitdate = max(clicks['ts']) - td(20)\n",
    "testsplitdate = max(clicks['ts']) - td(1)\n",
    "item_count = clicks['item'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remain_sess = []\n",
    "remain_item = set()\n",
    "for _, group in clicks.groupby('sess', sort=False):\n",
    "    print(group.iat[0, 0], end='\\r')\n",
    "    gi = group['item'].tolist()\n",
    "    n = len(gi)\n",
    "    stop = False\n",
    "    if n > 1:\n",
    "        for item in gi:\n",
    "            if item_count[item] < 5:\n",
    "                stop = True\n",
    "                break\n",
    "    else:\n",
    "        stop = True\n",
    "    if not stop:\n",
    "        remain_sess.append((str(group.iat[0, 0]), group.iat[0, 1], gi))\n",
    "        for item in gi:\n",
    "            remain_item.add(item)\n",
    "with open('../data/interim/n_items.json', 'w') as f:\n",
    "    json.dump(len(remain_item), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "remain_item = list(remain_item)\n",
    "item_enc = LabelEncoder()\n",
    "item_enc.fit(remain_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = defaultdict(list)\n",
    "val_d = defaultdict(list)\n",
    "test_d = defaultdict(list)\n",
    "for sess, ts, items in remain_sess:\n",
    "    print(sess, end='\\r')\n",
    "    items = item_enc.transform(items).tolist()\n",
    "    if ts < valsplitdate:\n",
    "        for i in range(1, len(items)):\n",
    "            train_d[sess].append((items[: i], items[i]))\n",
    "    elif ts < testsplitdate:\n",
    "        for i in range(1, len(items)):\n",
    "            val_d[sess].append((items[: i], items[i]))\n",
    "    else:\n",
    "        for i in range(1, len(items)):\n",
    "            test_d[sess].append((items[: i], items[i]))\n",
    "with open('../data/interim/train.json', 'w') as f:\n",
    "    json.dump(train_d, f)\n",
    "with open('../data/interim/val.json', 'w') as f:\n",
    "    json.dump(val_d, f)\n",
    "with open('../data/interim/test.json', 'w') as f:\n",
    "    json.dump(test_d, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from torch_geometric.data import Data, Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YooChooseDataset(Dataset):\n",
    "    def __init__(self, d):\n",
    "        super(YooChooseDataset, self).__init__()\n",
    "        self.samples = self.add_from_dict(d)\n",
    "        \n",
    "    def add_from_dict(self, d):\n",
    "        samples = []\n",
    "        for dd in d.values():\n",
    "            for x_ids, y in dd:\n",
    "                x_ids_ = set(x_ids)\n",
    "                x_ids_.remove(x_ids[- 1])\n",
    "                x_ids_ = list(x_ids_) + [x_ids[- 1]]\n",
    "                x_dict = {x_id: i for i, x_id in enumerate(x_ids_)}\n",
    "                x = [[x_id] for x_id in x_ids_]\n",
    "                edge_dict = defaultdict(lambda: defaultdict(int))\n",
    "                for i in range(len(x_ids) - 1):\n",
    "                    edge_dict[x_dict[x_ids[i]]][x_dict[x_ids[i + 1]]] += 1\n",
    "                edge_index, edge_weights = [], []\n",
    "                for o in edge_dict.keys():\n",
    "                    s = sum(edge_dict[o].values())\n",
    "                    for d in edge_dict[o].keys():\n",
    "                        edge_index.append([o, d])\n",
    "                        edge_weights.append(edge_dict[o][d] / s)\n",
    "                x = torch.tensor(x, dtype=torch.long)\n",
    "                edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "                edge_weights = torch.tensor(edge_weights)\n",
    "                samples.append((Data(x, edge_index=edge_index, edge_weights=edge_weights), y))\n",
    "        return samples\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "                'graph': self.samples[idx][0],\n",
    "                'label': self.samples[idx][1]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/interim/n_items.json', 'r') as f:\n",
    "    n_items = json.load(f)\n",
    "with open('../data/interim/train.json', 'r') as f:\n",
    "    train_d = json.load(f)\n",
    "with open('../data/interim/val.json', 'r') as f:\n",
    "    val_d = json.load(f)\n",
    "with open('../data/interim/test.json', 'r') as f:\n",
    "    test_d = json.load(f)\n",
    "print('# items: {}\\n# train sessions: {}\\n# val sessions: {}\\n# test sessions: {}'\n",
    "        .format(n_items, len(train_d), len(val_d), len(test_d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = YooChooseDataset(train_d)\n",
    "val_dataset = YooChooseDataset(val_d)\n",
    "test_dataset = YooChooseDataset(test_d)\n",
    "print('# train samples: {}\\n# val samples: {}\\n# test samples: {}'\n",
    "        .format(len(train_dataset), len(val_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "#from itertools import repeat\n",
    "\n",
    "def save_dataset(dataset, tvt, i, n_div):\n",
    "    lbd = floor((i / n_div) * len(dataset))\n",
    "    rbd = floor(((i + 1) / n_div) * len(dataset))\n",
    "    ds = YooChooseDataset({})\n",
    "    ds.samples = dataset.samples[lbd: rbd]\n",
    "    torch.save(ds, '../data/processed/{}_{}.pt'\n",
    "            .format(tvt, str(i).zfill(2)))\n",
    "\n",
    "n_div = 90\n",
    "for i in range(n_div):\n",
    "    save_dataset(train_dataset, 'train_dataset', i, n_div)\n",
    "n_div = 10\n",
    "for i in range(n_div):\n",
    "    save_dataset(val_dataset, 'val_dataset', i, n_div)\n",
    "n_div = 1\n",
    "for i in range(n_div):\n",
    "    save_dataset(test_dataset, 'test_dataset', i, n_div)\n",
    "    \n",
    "\"\"\"\n",
    "with Pool(30) as p:\n",
    "    dataset = train_dataset\n",
    "    tvt = 'train_dataset'\n",
    "    n_div = 90\n",
    "    p.map(save_dataset, zip(repeat(dataset), repeat(tvt), range(n_div), repeat(n_div)))\n",
    "    \n",
    "with Pool(30) as p:\n",
    "    dataset = val_dataset\n",
    "    tvt = 'val_dataset'\n",
    "    n_div = 10\n",
    "    p.map(save_dataset, zip(repeat(dataset), repeat(tvt), range(n_div), repeat(n_div)))\n",
    "\n",
    "with Pool(30) as p:\n",
    "    dataset = test_dataset\n",
    "    tvt = 'test_dataset'\n",
    "    n_div = 1\n",
    "    p.map(save_dataset, zip(repeat(dataset), repeat(tvt), range(n_div), repeat(n_div)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train samples: 2112000\n",
      "# val samples: 246752\n",
      "# test samples: 55474\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "with open('../data/interim/n_items.json', 'r') as f:\n",
    "    n_items = json.load(f)\n",
    "train_dataset_list = []\n",
    "for i in range(9):\n",
    "    d = torch.load(\n",
    "            '../data/processed/train_dataset_' + str(i).zfill(2) + '.pt')\n",
    "    train_dataset_list.append(d)\n",
    "train_dataset = ConcatDataset(train_dataset_list)\n",
    "val_dataset_list = []\n",
    "for i in range(1):\n",
    "    d = torch.load(\n",
    "            '../data/processed/val_dataset_' + str(i).zfill(2) + '.pt')\n",
    "    val_dataset_list.append(d)\n",
    "val_dataset = ConcatDataset(val_dataset_list)\n",
    "test_dataset_list = []\n",
    "for i in range(1):\n",
    "    d = torch.load(\n",
    "            '../data/processed/test_dataset_' + str(i).zfill(2) + '.pt')\n",
    "    test_dataset_list.append(d)\n",
    "test_dataset = ConcatDataset(test_dataset_list)\n",
    "print('# train samples: {}\\n# val samples: {}\\n# test samples: {}'\n",
    "        .format(len(train_dataset), len(val_dataset), len(test_dataset)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GatedGraphConv\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.wq = nn.Linear(embed_dim, embed_dim)\n",
    "        self.wk = nn.Linear(embed_dim, embed_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.q = nn.Linear(embed_dim, 1)\n",
    "        self.w = nn.Linear(2 * embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x, batch):\n",
    "        sections = list(torch.bincount(batch).to('cpu').numpy())\n",
    "        x_split = torch.split(x, sections)\n",
    "        q_split = [x_[- 1].view(1, - 1) for x_ in x_split]\n",
    "        q = torch.cat([x_[- 1].view(1, - 1).repeat(x_.shape[0], 1) for x_ in x_split])\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(x)\n",
    "        a = self.q(q + k)\n",
    "        ax = a * x\n",
    "        ax_split = torch.split(ax, sections)\n",
    "        sg_split = [torch.sum(ax_, 0).view(1, - 1) for ax_ in ax_split]\n",
    "        sh_split = self.w(torch.cat((torch.cat(q_split), torch.cat(sg_split)), 1))\n",
    "        return sh_split\n",
    "    \n",
    "class PredProb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PredProb, self).__init__()\n",
    "    \n",
    "    def forward(self, sh, embedding):\n",
    "        return torch.mm(sh, embedding.weight.transpose(1, 0))\n",
    "\n",
    "class SRGNN(nn.Module):\n",
    "    def __init__(self, n_items, embed_dim):\n",
    "        super(SRGNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_items, embed_dim)\n",
    "        self.gatedgconv = GatedGraphConv(embed_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.attention = Attention(embed_dim)\n",
    "        self.predprob = PredProb()\n",
    "        \n",
    "    def _initialize_weights(self, ):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weights, batch =\\\n",
    "                data.x, data.edge_index, data.edge_weights, data.batch\n",
    "        x = self.embedding(x).squeeze()\n",
    "        x = self.gatedgconv(x, edge_index, edge_weights)\n",
    "        x = self.relu(x)\n",
    "        x = self.attention(x, batch)\n",
    "        x = self.predprob(x, self.embedding)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('embedding', Embedding(37821, 128))\n",
      "('gatedgconv', GatedGraphConv(128, num_layers=1))\n",
      "('relu', ReLU())\n",
      "('attention', Attention(\n",
      "  (wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      "  (q): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (w): Linear(in_features=256, out_features=128, bias=True)\n",
      "))\n",
      "('predprob', PredProb())\n"
     ]
    }
   ],
   "source": [
    "model = SRGNN(n_items, 128)\n",
    "for b in model.named_children():\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.handlers import Checkpoint, DiskSaver\n",
    "\n",
    "def train_net(net, opt, loss_fn, val_metrics, train_loader, val_loader, device):\n",
    "    net.to(device)\n",
    "    def prepare_batch(batch, device, non_blocking=False):\n",
    "        x, y = batch.values()\n",
    "        return x.to(device), y.to(device)\n",
    "    def output_transform(x, y, y_pred, loss):\n",
    "        return (y_pred.max(1)[1], y)\n",
    "    trainer = create_supervised_trainer(net, opt, loss_fn, device,\n",
    "            prepare_batch=prepare_batch, output_transform=output_transform)\n",
    "    evaluator = create_supervised_evaluator(net, val_metrics, device,\n",
    "            prepare_batch=prepare_batch)\n",
    "    s = '{}: {:.2f} '\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(trainer):\n",
    "        evaluator.run(train_loader)\n",
    "        print('Epoch {}'.format(trainer.state.epoch))\n",
    "        message = 'Train - '\n",
    "        for m in val_metrics.keys():\n",
    "            message += s.format(m, evaluator.state.metrics[m])\n",
    "        print(message)\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_validation_results(trainer):\n",
    "        evaluator.run(val_loader)\n",
    "        message = 'Val   - '\n",
    "        for m in val_metrics.keys():\n",
    "            message += s.format(m, evaluator.state.metrics[m])\n",
    "        print(message)\n",
    "    \n",
    "    #=====================================\n",
    "    def score_function(engine):\n",
    "        return engine.state.metrics['top-10 acc']\n",
    "    to_save = {'model': model}\n",
    "    handler = Checkpoint(to_save, DiskSaver('../models/tmp', create_dir=True), n_saved=2,\n",
    "            filename_prefix='best', score_function=score_function, score_name=\"val_acc\")\n",
    "    evaluator.add_event_handler(Events.COMPLETED, handler)\n",
    "    #===============================\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train - top-10 acc: 0.06 loss: 10.30 \n",
      "Val   - top-10 acc: 0.00 loss: 11.25 \n",
      "Epoch 2\n",
      "Train - top-10 acc: 0.12 loss: 9.79 \n",
      "Val   - top-10 acc: 0.01 loss: 11.08 \n",
      "Epoch 3\n",
      "Train - top-10 acc: 0.17 loss: 9.41 \n",
      "Val   - top-10 acc: 0.01 loss: 10.97 \n",
      "Epoch 4\n",
      "Train - top-10 acc: 0.21 loss: 9.10 \n",
      "Val   - top-10 acc: 0.02 loss: 10.89 \n",
      "Epoch 5\n",
      "Train - top-10 acc: 0.24 loss: 8.85 \n",
      "Val   - top-10 acc: 0.03 loss: 10.84 \n",
      "Epoch 6\n",
      "Train - top-10 acc: 0.26 loss: 8.63 \n",
      "Val   - top-10 acc: 0.04 loss: 10.81 \n",
      "Epoch 7\n",
      "Train - top-10 acc: 0.28 loss: 8.44 \n",
      "Val   - top-10 acc: 0.05 loss: 10.79 \n",
      "Epoch 8\n",
      "Train - top-10 acc: 0.29 loss: 8.27 \n",
      "Val   - top-10 acc: 0.06 loss: 10.79 \n",
      "Epoch 9\n",
      "Train - top-10 acc: 0.31 loss: 8.11 \n",
      "Val   - top-10 acc: 0.06 loss: 10.79 \n",
      "Epoch 10\n",
      "Train - top-10 acc: 0.32 loss: 7.96 \n",
      "Val   - top-10 acc: 0.07 loss: 10.80 \n",
      "Epoch 11\n",
      "Train - top-10 acc: 0.33 loss: 7.82 \n",
      "Val   - top-10 acc: 0.07 loss: 10.82 \n",
      "Epoch 12\n",
      "Train - top-10 acc: 0.34 loss: 7.68 \n",
      "Val   - top-10 acc: 0.07 loss: 10.84 \n",
      "Epoch 13\n",
      "Train - top-10 acc: 0.35 loss: 7.55 \n",
      "Val   - top-10 acc: 0.08 loss: 10.86 \n",
      "Epoch 14\n",
      "Train - top-10 acc: 0.36 loss: 7.43 \n",
      "Val   - top-10 acc: 0.08 loss: 10.87 \n",
      "Epoch 15\n",
      "Train - top-10 acc: 0.37 loss: 7.31 \n",
      "Val   - top-10 acc: 0.08 loss: 10.88 \n",
      "Epoch 16\n",
      "Train - top-10 acc: 0.37 loss: 7.20 \n",
      "Val   - top-10 acc: 0.08 loss: 10.89 \n",
      "Epoch 17\n",
      "Train - top-10 acc: 0.38 loss: 7.10 \n",
      "Val   - top-10 acc: 0.08 loss: 10.90 \n",
      "Epoch 18\n",
      "Train - top-10 acc: 0.38 loss: 7.00 \n",
      "Val   - top-10 acc: 0.09 loss: 10.90 \n",
      "Epoch 19\n",
      "Train - top-10 acc: 0.39 loss: 6.90 \n",
      "Val   - top-10 acc: 0.09 loss: 10.90 \n",
      "Epoch 20\n",
      "Train - top-10 acc: 0.40 loss: 6.82 \n",
      "Val   - top-10 acc: 0.09 loss: 10.89 \n",
      "Epoch 21\n",
      "Train - top-10 acc: 0.40 loss: 6.74 \n",
      "Val   - top-10 acc: 0.10 loss: 10.88 \n",
      "Epoch 22\n",
      "Train - top-10 acc: 0.41 loss: 6.66 \n",
      "Val   - top-10 acc: 0.10 loss: 10.87 \n",
      "Epoch 23\n",
      "Train - top-10 acc: 0.41 loss: 6.59 \n",
      "Val   - top-10 acc: 0.10 loss: 10.86 \n",
      "Epoch 24\n",
      "Train - top-10 acc: 0.42 loss: 6.52 \n",
      "Val   - top-10 acc: 0.10 loss: 10.85 \n",
      "Epoch 25\n",
      "Train - top-10 acc: 0.42 loss: 6.46 \n",
      "Val   - top-10 acc: 0.11 loss: 10.84 \n",
      "Epoch 26\n",
      "Train - top-10 acc: 0.42 loss: 6.40 \n",
      "Val   - top-10 acc: 0.11 loss: 10.82 \n",
      "Epoch 27\n",
      "Train - top-10 acc: 0.43 loss: 6.35 \n",
      "Val   - top-10 acc: 0.12 loss: 10.80 \n",
      "Epoch 28\n",
      "Train - top-10 acc: 0.43 loss: 6.30 \n",
      "Val   - top-10 acc: 0.12 loss: 10.79 \n",
      "Epoch 29\n",
      "Train - top-10 acc: 0.43 loss: 6.25 \n",
      "Val   - top-10 acc: 0.12 loss: 10.78 \n",
      "Epoch 30\n",
      "Train - top-10 acc: 0.44 loss: 6.21 \n",
      "Val   - top-10 acc: 0.13 loss: 10.76 \n",
      "Epoch 31\n",
      "Train - top-10 acc: 0.44 loss: 6.17 \n",
      "Val   - top-10 acc: 0.13 loss: 10.74 \n",
      "Epoch 32\n",
      "Train - top-10 acc: 0.44 loss: 6.13 \n",
      "Val   - top-10 acc: 0.13 loss: 10.73 \n",
      "Epoch 33\n",
      "Train - top-10 acc: 0.44 loss: 6.10 \n",
      "Val   - top-10 acc: 0.14 loss: 10.71 \n",
      "Epoch 34\n",
      "Train - top-10 acc: 0.45 loss: 6.07 \n",
      "Val   - top-10 acc: 0.14 loss: 10.70 \n",
      "Epoch 35\n",
      "Train - top-10 acc: 0.45 loss: 6.03 \n",
      "Val   - top-10 acc: 0.14 loss: 10.69 \n",
      "Epoch 36\n",
      "Train - top-10 acc: 0.45 loss: 6.01 \n",
      "Val   - top-10 acc: 0.14 loss: 10.68 \n",
      "Epoch 37\n",
      "Train - top-10 acc: 0.45 loss: 5.98 \n",
      "Val   - top-10 acc: 0.15 loss: 10.66 \n",
      "Epoch 38\n",
      "Train - top-10 acc: 0.45 loss: 5.95 \n",
      "Val   - top-10 acc: 0.15 loss: 10.66 \n",
      "Epoch 39\n",
      "Train - top-10 acc: 0.46 loss: 5.93 \n",
      "Val   - top-10 acc: 0.15 loss: 10.65 \n",
      "Epoch 40\n",
      "Train - top-10 acc: 0.46 loss: 5.90 \n",
      "Val   - top-10 acc: 0.15 loss: 10.64 \n",
      "Epoch 41\n",
      "Train - top-10 acc: 0.46 loss: 5.88 \n",
      "Val   - top-10 acc: 0.15 loss: 10.63 \n",
      "Epoch 42\n",
      "Train - top-10 acc: 0.46 loss: 5.86 \n",
      "Val   - top-10 acc: 0.15 loss: 10.62 \n",
      "Epoch 43\n",
      "Train - top-10 acc: 0.46 loss: 5.84 \n",
      "Val   - top-10 acc: 0.16 loss: 10.61 \n",
      "Epoch 44\n",
      "Train - top-10 acc: 0.47 loss: 5.82 \n",
      "Val   - top-10 acc: 0.16 loss: 10.61 \n",
      "Epoch 45\n",
      "Train - top-10 acc: 0.47 loss: 5.80 \n",
      "Val   - top-10 acc: 0.16 loss: 10.60 \n",
      "Epoch 46\n",
      "Train - top-10 acc: 0.47 loss: 5.78 \n",
      "Val   - top-10 acc: 0.16 loss: 10.60 \n",
      "Epoch 47\n",
      "Train - top-10 acc: 0.47 loss: 5.77 \n",
      "Val   - top-10 acc: 0.16 loss: 10.59 \n",
      "Epoch 48\n",
      "Train - top-10 acc: 0.47 loss: 5.75 \n",
      "Val   - top-10 acc: 0.16 loss: 10.59 \n",
      "Epoch 49\n",
      "Train - top-10 acc: 0.47 loss: 5.74 \n",
      "Val   - top-10 acc: 0.16 loss: 10.60 \n",
      "Epoch 50\n",
      "Train - top-10 acc: 0.48 loss: 5.72 \n",
      "Val   - top-10 acc: 0.16 loss: 10.59 \n",
      "Epoch 51\n",
      "Train - top-10 acc: 0.48 loss: 5.71 \n",
      "Val   - top-10 acc: 0.16 loss: 10.59 \n",
      "Epoch 52\n",
      "Train - top-10 acc: 0.48 loss: 5.69 \n",
      "Val   - top-10 acc: 0.16 loss: 10.59 \n",
      "Epoch 53\n",
      "Train - top-10 acc: 0.48 loss: 5.68 \n",
      "Val   - top-10 acc: 0.16 loss: 10.59 \n",
      "Epoch 54\n",
      "Train - top-10 acc: 0.48 loss: 5.66 \n",
      "Val   - top-10 acc: 0.16 loss: 10.59 \n",
      "Epoch 55\n",
      "Train - top-10 acc: 0.48 loss: 5.65 \n",
      "Val   - top-10 acc: 0.16 loss: 10.59 \n",
      "Epoch 56\n",
      "Train - top-10 acc: 0.48 loss: 5.64 \n",
      "Val   - top-10 acc: 0.16 loss: 10.60 \n",
      "Epoch 57\n",
      "Train - top-10 acc: 0.48 loss: 5.63 \n",
      "Val   - top-10 acc: 0.16 loss: 10.60 \n",
      "Epoch 58\n",
      "Train - top-10 acc: 0.49 loss: 5.61 \n",
      "Val   - top-10 acc: 0.17 loss: 10.59 \n",
      "Epoch 59\n",
      "Train - top-10 acc: 0.49 loss: 5.60 \n",
      "Val   - top-10 acc: 0.17 loss: 10.60 \n",
      "Epoch 60\n",
      "Train - top-10 acc: 0.49 loss: 5.59 \n",
      "Val   - top-10 acc: 0.17 loss: 10.59 \n",
      "Epoch 61\n",
      "Train - top-10 acc: 0.49 loss: 5.58 \n",
      "Val   - top-10 acc: 0.17 loss: 10.60 \n",
      "Epoch 62\n",
      "Train - top-10 acc: 0.49 loss: 5.57 \n",
      "Val   - top-10 acc: 0.17 loss: 10.61 \n",
      "Epoch 63\n",
      "Train - top-10 acc: 0.49 loss: 5.56 \n",
      "Val   - top-10 acc: 0.17 loss: 10.60 \n",
      "Epoch 64\n",
      "Train - top-10 acc: 0.49 loss: 5.55 \n",
      "Val   - top-10 acc: 0.17 loss: 10.61 \n",
      "Epoch 65\n",
      "Train - top-10 acc: 0.49 loss: 5.54 \n",
      "Val   - top-10 acc: 0.17 loss: 10.62 \n",
      "Epoch 66\n",
      "Train - top-10 acc: 0.49 loss: 5.53 \n",
      "Val   - top-10 acc: 0.17 loss: 10.62 \n",
      "Epoch 67\n",
      "Train - top-10 acc: 0.50 loss: 5.52 \n",
      "Val   - top-10 acc: 0.17 loss: 10.63 \n",
      "Epoch 68\n",
      "Train - top-10 acc: 0.50 loss: 5.51 \n",
      "Val   - top-10 acc: 0.17 loss: 10.63 \n",
      "Epoch 69\n",
      "Train - top-10 acc: 0.50 loss: 5.50 \n",
      "Val   - top-10 acc: 0.17 loss: 10.64 \n",
      "Epoch 70\n",
      "Train - top-10 acc: 0.50 loss: 5.49 \n",
      "Val   - top-10 acc: 0.17 loss: 10.64 \n",
      "Epoch 71\n",
      "Train - top-10 acc: 0.50 loss: 5.48 \n",
      "Val   - top-10 acc: 0.17 loss: 10.64 \n",
      "Epoch 72\n",
      "Train - top-10 acc: 0.50 loss: 5.47 \n",
      "Val   - top-10 acc: 0.17 loss: 10.65 \n",
      "Epoch 73\n",
      "Train - top-10 acc: 0.50 loss: 5.46 \n",
      "Val   - top-10 acc: 0.17 loss: 10.65 \n",
      "Epoch 74\n",
      "Train - top-10 acc: 0.50 loss: 5.45 \n",
      "Val   - top-10 acc: 0.17 loss: 10.66 \n",
      "Epoch 75\n",
      "Train - top-10 acc: 0.50 loss: 5.44 \n",
      "Val   - top-10 acc: 0.17 loss: 10.66 \n",
      "Epoch 76\n",
      "Train - top-10 acc: 0.50 loss: 5.43 \n",
      "Val   - top-10 acc: 0.17 loss: 10.66 \n",
      "Epoch 77\n",
      "Train - top-10 acc: 0.50 loss: 5.42 \n",
      "Val   - top-10 acc: 0.17 loss: 10.67 \n",
      "Epoch 78\n",
      "Train - top-10 acc: 0.51 loss: 5.41 \n",
      "Val   - top-10 acc: 0.17 loss: 10.67 \n",
      "Epoch 79\n",
      "Train - top-10 acc: 0.51 loss: 5.41 \n",
      "Val   - top-10 acc: 0.17 loss: 10.68 \n",
      "Epoch 80\n",
      "Train - top-10 acc: 0.51 loss: 5.40 \n",
      "Val   - top-10 acc: 0.17 loss: 10.69 \n",
      "Epoch 81\n",
      "Train - top-10 acc: 0.51 loss: 5.39 \n",
      "Val   - top-10 acc: 0.17 loss: 10.69 \n",
      "Epoch 82\n",
      "Train - top-10 acc: 0.51 loss: 5.38 \n",
      "Val   - top-10 acc: 0.17 loss: 10.70 \n",
      "Epoch 83\n",
      "Train - top-10 acc: 0.51 loss: 5.37 \n",
      "Val   - top-10 acc: 0.17 loss: 10.70 \n",
      "Epoch 84\n",
      "Train - top-10 acc: 0.51 loss: 5.37 \n",
      "Val   - top-10 acc: 0.17 loss: 10.70 \n",
      "Epoch 85\n",
      "Train - top-10 acc: 0.51 loss: 5.36 \n",
      "Val   - top-10 acc: 0.17 loss: 10.71 \n",
      "Epoch 86\n",
      "Train - top-10 acc: 0.51 loss: 5.35 \n",
      "Val   - top-10 acc: 0.17 loss: 10.71 \n",
      "Epoch 87\n",
      "Train - top-10 acc: 0.51 loss: 5.34 \n",
      "Val   - top-10 acc: 0.17 loss: 10.72 \n",
      "Epoch 88\n",
      "Train - top-10 acc: 0.51 loss: 5.33 \n",
      "Val   - top-10 acc: 0.17 loss: 10.72 \n",
      "Epoch 89\n",
      "Train - top-10 acc: 0.52 loss: 5.33 \n",
      "Val   - top-10 acc: 0.16 loss: 10.73 \n",
      "Epoch 90\n",
      "Train - top-10 acc: 0.52 loss: 5.32 \n",
      "Val   - top-10 acc: 0.16 loss: 10.74 \n",
      "Epoch 91\n",
      "Train - top-10 acc: 0.52 loss: 5.31 \n",
      "Val   - top-10 acc: 0.16 loss: 10.75 \n",
      "Epoch 92\n",
      "Train - top-10 acc: 0.52 loss: 5.30 \n",
      "Val   - top-10 acc: 0.16 loss: 10.75 \n",
      "Epoch 93\n",
      "Train - top-10 acc: 0.52 loss: 5.30 \n",
      "Val   - top-10 acc: 0.16 loss: 10.76 \n",
      "Epoch 94\n",
      "Train - top-10 acc: 0.52 loss: 5.29 \n",
      "Val   - top-10 acc: 0.16 loss: 10.76 \n",
      "Epoch 95\n",
      "Train - top-10 acc: 0.52 loss: 5.28 \n",
      "Val   - top-10 acc: 0.16 loss: 10.76 \n",
      "Epoch 96\n",
      "Train - top-10 acc: 0.52 loss: 5.28 \n",
      "Val   - top-10 acc: 0.16 loss: 10.77 \n",
      "Epoch 97\n",
      "Train - top-10 acc: 0.52 loss: 5.27 \n",
      "Val   - top-10 acc: 0.16 loss: 10.77 \n",
      "Epoch 98\n",
      "Train - top-10 acc: 0.52 loss: 5.26 \n",
      "Val   - top-10 acc: 0.16 loss: 10.78 \n",
      "Epoch 99\n",
      "Train - top-10 acc: 0.52 loss: 5.25 \n",
      "Val   - top-10 acc: 0.16 loss: 10.78 \n",
      "Epoch 100\n",
      "Train - top-10 acc: 0.52 loss: 5.25 \n",
      "Val   - top-10 acc: 0.16 loss: 10.79 \n",
      "Epoch 101\n",
      "Train - top-10 acc: 0.52 loss: 5.24 \n",
      "Val   - top-10 acc: 0.16 loss: 10.79 \n",
      "Epoch 102\n",
      "Train - top-10 acc: 0.53 loss: 5.23 \n",
      "Val   - top-10 acc: 0.16 loss: 10.80 \n",
      "Epoch 103\n",
      "Train - top-10 acc: 0.53 loss: 5.23 \n",
      "Val   - top-10 acc: 0.16 loss: 10.80 \n",
      "Epoch 104\n",
      "Train - top-10 acc: 0.53 loss: 5.22 \n",
      "Val   - top-10 acc: 0.16 loss: 10.80 \n",
      "Epoch 105\n",
      "Train - top-10 acc: 0.53 loss: 5.21 \n",
      "Val   - top-10 acc: 0.16 loss: 10.81 \n",
      "Epoch 106\n",
      "Train - top-10 acc: 0.53 loss: 5.21 \n",
      "Val   - top-10 acc: 0.16 loss: 10.82 \n",
      "Epoch 107\n",
      "Train - top-10 acc: 0.53 loss: 5.20 \n",
      "Val   - top-10 acc: 0.16 loss: 10.82 \n",
      "Epoch 108\n",
      "Train - top-10 acc: 0.53 loss: 5.20 \n",
      "Val   - top-10 acc: 0.16 loss: 10.83 \n",
      "Epoch 109\n",
      "Train - top-10 acc: 0.53 loss: 5.19 \n",
      "Val   - top-10 acc: 0.16 loss: 10.84 \n",
      "Epoch 110\n",
      "Train - top-10 acc: 0.53 loss: 5.18 \n",
      "Val   - top-10 acc: 0.16 loss: 10.84 \n",
      "Epoch 111\n",
      "Train - top-10 acc: 0.53 loss: 5.18 \n",
      "Val   - top-10 acc: 0.16 loss: 10.84 \n",
      "Epoch 112\n",
      "Train - top-10 acc: 0.53 loss: 5.17 \n",
      "Val   - top-10 acc: 0.16 loss: 10.85 \n",
      "Epoch 113\n",
      "Train - top-10 acc: 0.53 loss: 5.16 \n",
      "Val   - top-10 acc: 0.16 loss: 10.85 \n",
      "Epoch 114\n",
      "Train - top-10 acc: 0.53 loss: 5.16 \n",
      "Val   - top-10 acc: 0.16 loss: 10.86 \n",
      "Epoch 115\n",
      "Train - top-10 acc: 0.53 loss: 5.15 \n",
      "Val   - top-10 acc: 0.16 loss: 10.86 \n",
      "Epoch 116\n",
      "Train - top-10 acc: 0.53 loss: 5.15 \n",
      "Val   - top-10 acc: 0.16 loss: 10.86 \n",
      "Epoch 117\n",
      "Train - top-10 acc: 0.54 loss: 5.14 \n",
      "Val   - top-10 acc: 0.16 loss: 10.87 \n",
      "Epoch 118\n",
      "Train - top-10 acc: 0.54 loss: 5.13 \n",
      "Val   - top-10 acc: 0.16 loss: 10.87 \n",
      "Epoch 119\n",
      "Train - top-10 acc: 0.54 loss: 5.13 \n",
      "Val   - top-10 acc: 0.16 loss: 10.87 \n",
      "Epoch 120\n",
      "Train - top-10 acc: 0.54 loss: 5.12 \n",
      "Val   - top-10 acc: 0.16 loss: 10.88 \n",
      "Epoch 121\n",
      "Train - top-10 acc: 0.54 loss: 5.12 \n",
      "Val   - top-10 acc: 0.16 loss: 10.88 \n",
      "Epoch 122\n",
      "Train - top-10 acc: 0.54 loss: 5.11 \n",
      "Val   - top-10 acc: 0.16 loss: 10.89 \n",
      "Epoch 123\n",
      "Train - top-10 acc: 0.54 loss: 5.10 \n",
      "Val   - top-10 acc: 0.16 loss: 10.89 \n",
      "Epoch 124\n",
      "Train - top-10 acc: 0.54 loss: 5.10 \n",
      "Val   - top-10 acc: 0.16 loss: 10.90 \n",
      "Epoch 125\n",
      "Train - top-10 acc: 0.54 loss: 5.09 \n",
      "Val   - top-10 acc: 0.16 loss: 10.90 \n",
      "Epoch 126\n",
      "Train - top-10 acc: 0.54 loss: 5.09 \n",
      "Val   - top-10 acc: 0.16 loss: 10.90 \n",
      "Epoch 127\n",
      "Train - top-10 acc: 0.54 loss: 5.08 \n",
      "Val   - top-10 acc: 0.16 loss: 10.91 \n",
      "Epoch 128\n",
      "Train - top-10 acc: 0.54 loss: 5.08 \n",
      "Val   - top-10 acc: 0.16 loss: 10.91 \n",
      "Epoch 129\n",
      "Train - top-10 acc: 0.54 loss: 5.07 \n",
      "Val   - top-10 acc: 0.15 loss: 10.92 \n",
      "Epoch 130\n",
      "Train - top-10 acc: 0.54 loss: 5.06 \n",
      "Val   - top-10 acc: 0.16 loss: 10.92 \n",
      "Epoch 131\n",
      "Train - top-10 acc: 0.54 loss: 5.06 \n",
      "Val   - top-10 acc: 0.15 loss: 10.93 \n",
      "Epoch 132\n",
      "Train - top-10 acc: 0.54 loss: 5.05 \n",
      "Val   - top-10 acc: 0.15 loss: 10.94 \n",
      "Epoch 133\n",
      "Train - top-10 acc: 0.55 loss: 5.05 \n",
      "Val   - top-10 acc: 0.15 loss: 10.93 \n",
      "Epoch 134\n",
      "Train - top-10 acc: 0.55 loss: 5.04 \n",
      "Val   - top-10 acc: 0.15 loss: 10.93 \n",
      "Epoch 135\n",
      "Train - top-10 acc: 0.55 loss: 5.04 \n",
      "Val   - top-10 acc: 0.15 loss: 10.94 \n",
      "Epoch 136\n",
      "Train - top-10 acc: 0.55 loss: 5.03 \n",
      "Val   - top-10 acc: 0.15 loss: 10.95 \n",
      "Epoch 137\n",
      "Train - top-10 acc: 0.55 loss: 5.03 \n",
      "Val   - top-10 acc: 0.15 loss: 10.94 \n",
      "Epoch 138\n",
      "Train - top-10 acc: 0.55 loss: 5.02 \n",
      "Val   - top-10 acc: 0.15 loss: 10.95 \n",
      "Epoch 139\n",
      "Train - top-10 acc: 0.55 loss: 5.02 \n",
      "Val   - top-10 acc: 0.15 loss: 10.95 \n",
      "Epoch 140\n",
      "Train - top-10 acc: 0.55 loss: 5.01 \n",
      "Val   - top-10 acc: 0.15 loss: 10.96 \n",
      "Epoch 141\n",
      "Train - top-10 acc: 0.55 loss: 5.01 \n",
      "Val   - top-10 acc: 0.15 loss: 10.97 \n",
      "Epoch 142\n",
      "Train - top-10 acc: 0.55 loss: 5.00 \n",
      "Val   - top-10 acc: 0.15 loss: 10.97 \n",
      "Epoch 143\n",
      "Train - top-10 acc: 0.55 loss: 5.00 \n",
      "Val   - top-10 acc: 0.15 loss: 10.96 \n",
      "Epoch 144\n",
      "Train - top-10 acc: 0.55 loss: 4.99 \n",
      "Val   - top-10 acc: 0.15 loss: 10.97 \n",
      "Epoch 145\n",
      "Train - top-10 acc: 0.55 loss: 4.99 \n",
      "Val   - top-10 acc: 0.15 loss: 10.97 \n",
      "Epoch 146\n",
      "Train - top-10 acc: 0.55 loss: 4.98 \n",
      "Val   - top-10 acc: 0.15 loss: 10.98 \n",
      "Epoch 147\n",
      "Train - top-10 acc: 0.55 loss: 4.97 \n",
      "Val   - top-10 acc: 0.15 loss: 10.98 \n",
      "Epoch 148\n",
      "Train - top-10 acc: 0.55 loss: 4.97 \n",
      "Val   - top-10 acc: 0.15 loss: 10.99 \n",
      "Epoch 149\n",
      "Train - top-10 acc: 0.55 loss: 4.97 \n",
      "Val   - top-10 acc: 0.15 loss: 10.99 \n",
      "Epoch 150\n",
      "Train - top-10 acc: 0.55 loss: 4.96 \n",
      "Val   - top-10 acc: 0.15 loss: 10.99 \n",
      "Epoch 151\n",
      "Train - top-10 acc: 0.55 loss: 4.96 \n",
      "Val   - top-10 acc: 0.15 loss: 10.99 \n",
      "Epoch 152\n",
      "Train - top-10 acc: 0.56 loss: 4.95 \n",
      "Val   - top-10 acc: 0.15 loss: 10.99 \n",
      "Epoch 153\n",
      "Train - top-10 acc: 0.56 loss: 4.95 \n",
      "Val   - top-10 acc: 0.15 loss: 11.00 \n",
      "Epoch 154\n",
      "Train - top-10 acc: 0.56 loss: 4.94 \n",
      "Val   - top-10 acc: 0.15 loss: 11.00 \n",
      "Epoch 155\n",
      "Train - top-10 acc: 0.56 loss: 4.94 \n",
      "Val   - top-10 acc: 0.15 loss: 11.01 \n",
      "Epoch 156\n",
      "Train - top-10 acc: 0.56 loss: 4.93 \n",
      "Val   - top-10 acc: 0.15 loss: 11.01 \n",
      "Epoch 157\n",
      "Train - top-10 acc: 0.56 loss: 4.93 \n",
      "Val   - top-10 acc: 0.15 loss: 11.01 \n",
      "Epoch 158\n",
      "Train - top-10 acc: 0.56 loss: 4.92 \n",
      "Val   - top-10 acc: 0.15 loss: 11.01 \n",
      "Epoch 159\n",
      "Train - top-10 acc: 0.56 loss: 4.92 \n",
      "Val   - top-10 acc: 0.15 loss: 11.02 \n",
      "Epoch 160\n",
      "Train - top-10 acc: 0.56 loss: 4.91 \n",
      "Val   - top-10 acc: 0.14 loss: 11.02 \n",
      "Epoch 161\n",
      "Train - top-10 acc: 0.56 loss: 4.91 \n",
      "Val   - top-10 acc: 0.14 loss: 11.02 \n",
      "Epoch 162\n",
      "Train - top-10 acc: 0.56 loss: 4.90 \n",
      "Val   - top-10 acc: 0.14 loss: 11.03 \n",
      "Epoch 163\n",
      "Train - top-10 acc: 0.56 loss: 4.90 \n",
      "Val   - top-10 acc: 0.14 loss: 11.02 \n",
      "Epoch 164\n",
      "Train - top-10 acc: 0.56 loss: 4.89 \n",
      "Val   - top-10 acc: 0.14 loss: 11.03 \n",
      "Epoch 165\n",
      "Train - top-10 acc: 0.56 loss: 4.89 \n",
      "Val   - top-10 acc: 0.14 loss: 11.03 \n",
      "Epoch 166\n",
      "Train - top-10 acc: 0.56 loss: 4.88 \n",
      "Val   - top-10 acc: 0.14 loss: 11.03 \n",
      "Epoch 167\n",
      "Train - top-10 acc: 0.56 loss: 4.88 \n",
      "Val   - top-10 acc: 0.14 loss: 11.04 \n",
      "Epoch 168\n",
      "Train - top-10 acc: 0.56 loss: 4.88 \n",
      "Val   - top-10 acc: 0.14 loss: 11.04 \n",
      "Epoch 169\n",
      "Train - top-10 acc: 0.56 loss: 4.87 \n",
      "Val   - top-10 acc: 0.14 loss: 11.05 \n",
      "Epoch 170\n",
      "Train - top-10 acc: 0.56 loss: 4.87 \n",
      "Val   - top-10 acc: 0.14 loss: 11.05 \n",
      "Epoch 171\n",
      "Train - top-10 acc: 0.56 loss: 4.86 \n",
      "Val   - top-10 acc: 0.14 loss: 11.04 \n",
      "Epoch 172\n",
      "Train - top-10 acc: 0.56 loss: 4.86 \n",
      "Val   - top-10 acc: 0.14 loss: 11.04 \n",
      "Epoch 173\n",
      "Train - top-10 acc: 0.57 loss: 4.85 \n",
      "Val   - top-10 acc: 0.14 loss: 11.05 \n",
      "Epoch 174\n",
      "Train - top-10 acc: 0.57 loss: 4.85 \n",
      "Val   - top-10 acc: 0.14 loss: 11.05 \n",
      "Epoch 175\n",
      "Train - top-10 acc: 0.57 loss: 4.84 \n",
      "Val   - top-10 acc: 0.14 loss: 11.06 \n",
      "Epoch 176\n",
      "Train - top-10 acc: 0.57 loss: 4.84 \n",
      "Val   - top-10 acc: 0.14 loss: 11.06 \n",
      "Epoch 177\n",
      "Train - top-10 acc: 0.57 loss: 4.84 \n",
      "Val   - top-10 acc: 0.14 loss: 11.06 \n",
      "Epoch 178\n",
      "Train - top-10 acc: 0.57 loss: 4.83 \n",
      "Val   - top-10 acc: 0.14 loss: 11.06 \n",
      "Epoch 179\n",
      "Train - top-10 acc: 0.57 loss: 4.83 \n",
      "Val   - top-10 acc: 0.14 loss: 11.06 \n",
      "Epoch 180\n",
      "Train - top-10 acc: 0.57 loss: 4.82 \n",
      "Val   - top-10 acc: 0.14 loss: 11.06 \n",
      "Epoch 181\n",
      "Train - top-10 acc: 0.57 loss: 4.82 \n",
      "Val   - top-10 acc: 0.14 loss: 11.07 \n",
      "Epoch 182\n",
      "Train - top-10 acc: 0.57 loss: 4.81 \n",
      "Val   - top-10 acc: 0.14 loss: 11.07 \n",
      "Epoch 183\n",
      "Train - top-10 acc: 0.57 loss: 4.81 \n",
      "Val   - top-10 acc: 0.14 loss: 11.07 \n",
      "Epoch 184\n",
      "Train - top-10 acc: 0.57 loss: 4.81 \n",
      "Val   - top-10 acc: 0.14 loss: 11.07 \n",
      "Epoch 185\n",
      "Train - top-10 acc: 0.57 loss: 4.80 \n",
      "Val   - top-10 acc: 0.14 loss: 11.08 \n",
      "Epoch 186\n",
      "Train - top-10 acc: 0.57 loss: 4.80 \n",
      "Val   - top-10 acc: 0.14 loss: 11.09 \n",
      "Epoch 187\n",
      "Train - top-10 acc: 0.57 loss: 4.79 \n",
      "Val   - top-10 acc: 0.14 loss: 11.08 \n",
      "Epoch 188\n",
      "Train - top-10 acc: 0.57 loss: 4.79 \n",
      "Val   - top-10 acc: 0.14 loss: 11.08 \n",
      "Epoch 189\n",
      "Train - top-10 acc: 0.57 loss: 4.79 \n",
      "Val   - top-10 acc: 0.14 loss: 11.08 \n",
      "Epoch 190\n",
      "Train - top-10 acc: 0.57 loss: 4.78 \n",
      "Val   - top-10 acc: 0.14 loss: 11.08 \n",
      "Epoch 191\n",
      "Train - top-10 acc: 0.57 loss: 4.78 \n",
      "Val   - top-10 acc: 0.14 loss: 11.09 \n",
      "Epoch 192\n",
      "Train - top-10 acc: 0.57 loss: 4.77 \n",
      "Val   - top-10 acc: 0.14 loss: 11.08 \n",
      "Epoch 193\n",
      "Train - top-10 acc: 0.57 loss: 4.77 \n",
      "Val   - top-10 acc: 0.14 loss: 11.08 \n",
      "Epoch 194\n",
      "Train - top-10 acc: 0.57 loss: 4.77 \n",
      "Val   - top-10 acc: 0.13 loss: 11.09 \n",
      "Epoch 195\n",
      "Train - top-10 acc: 0.57 loss: 4.76 \n",
      "Val   - top-10 acc: 0.13 loss: 11.09 \n",
      "Epoch 196\n",
      "Train - top-10 acc: 0.58 loss: 4.76 \n",
      "Val   - top-10 acc: 0.13 loss: 11.10 \n",
      "Epoch 197\n",
      "Train - top-10 acc: 0.58 loss: 4.75 \n",
      "Val   - top-10 acc: 0.13 loss: 11.09 \n",
      "Epoch 198\n",
      "Train - top-10 acc: 0.58 loss: 4.75 \n",
      "Val   - top-10 acc: 0.13 loss: 11.10 \n",
      "Epoch 199\n",
      "Train - top-10 acc: 0.58 loss: 4.75 \n",
      "Val   - top-10 acc: 0.13 loss: 11.11 \n",
      "Epoch 200\n",
      "Train - top-10 acc: 0.58 loss: 4.74 \n",
      "Val   - top-10 acc: 0.13 loss: 11.11 \n",
      "Epoch 201\n",
      "Train - top-10 acc: 0.58 loss: 4.74 \n",
      "Val   - top-10 acc: 0.13 loss: 11.10 \n",
      "Epoch 202\n",
      "Train - top-10 acc: 0.58 loss: 4.73 \n",
      "Val   - top-10 acc: 0.13 loss: 11.10 \n",
      "Epoch 203\n",
      "Train - top-10 acc: 0.58 loss: 4.73 \n",
      "Val   - top-10 acc: 0.13 loss: 11.11 \n",
      "Epoch 204\n",
      "Train - top-10 acc: 0.58 loss: 4.73 \n",
      "Val   - top-10 acc: 0.13 loss: 11.12 \n",
      "Epoch 205\n",
      "Train - top-10 acc: 0.58 loss: 4.72 \n",
      "Val   - top-10 acc: 0.13 loss: 11.12 \n",
      "Epoch 206\n",
      "Train - top-10 acc: 0.58 loss: 4.72 \n",
      "Val   - top-10 acc: 0.13 loss: 11.11 \n",
      "Epoch 207\n",
      "Train - top-10 acc: 0.58 loss: 4.72 \n",
      "Val   - top-10 acc: 0.13 loss: 11.11 \n",
      "Epoch 208\n",
      "Train - top-10 acc: 0.58 loss: 4.71 \n",
      "Val   - top-10 acc: 0.13 loss: 11.11 \n",
      "Epoch 209\n",
      "Train - top-10 acc: 0.58 loss: 4.71 \n",
      "Val   - top-10 acc: 0.13 loss: 11.12 \n",
      "Epoch 210\n",
      "Train - top-10 acc: 0.58 loss: 4.70 \n",
      "Val   - top-10 acc: 0.13 loss: 11.12 \n",
      "Epoch 211\n",
      "Train - top-10 acc: 0.58 loss: 4.70 \n",
      "Val   - top-10 acc: 0.13 loss: 11.12 \n",
      "Epoch 212\n",
      "Train - top-10 acc: 0.58 loss: 4.70 \n",
      "Val   - top-10 acc: 0.13 loss: 11.12 \n",
      "Epoch 213\n",
      "Train - top-10 acc: 0.58 loss: 4.69 \n",
      "Val   - top-10 acc: 0.13 loss: 11.12 \n",
      "Epoch 214\n",
      "Train - top-10 acc: 0.58 loss: 4.69 \n",
      "Val   - top-10 acc: 0.13 loss: 11.12 \n",
      "Epoch 215\n",
      "Train - top-10 acc: 0.58 loss: 4.69 \n",
      "Val   - top-10 acc: 0.13 loss: 11.12 \n",
      "Epoch 216\n",
      "Train - top-10 acc: 0.58 loss: 4.68 \n",
      "Val   - top-10 acc: 0.13 loss: 11.13 \n",
      "Epoch 217\n",
      "Train - top-10 acc: 0.58 loss: 4.68 \n",
      "Val   - top-10 acc: 0.13 loss: 11.12 \n",
      "Epoch 218\n",
      "Train - top-10 acc: 0.58 loss: 4.68 \n",
      "Val   - top-10 acc: 0.13 loss: 11.13 \n",
      "Epoch 219\n",
      "Train - top-10 acc: 0.58 loss: 4.67 \n",
      "Val   - top-10 acc: 0.13 loss: 11.13 \n",
      "Epoch 220\n",
      "Train - top-10 acc: 0.58 loss: 4.67 \n",
      "Val   - top-10 acc: 0.13 loss: 11.13 \n",
      "Epoch 221\n",
      "Train - top-10 acc: 0.58 loss: 4.67 \n",
      "Val   - top-10 acc: 0.13 loss: 11.14 \n",
      "Epoch 222\n",
      "Train - top-10 acc: 0.58 loss: 4.66 \n",
      "Val   - top-10 acc: 0.13 loss: 11.14 \n",
      "Epoch 223\n",
      "Train - top-10 acc: 0.59 loss: 4.66 \n",
      "Val   - top-10 acc: 0.13 loss: 11.14 \n",
      "Epoch 224\n",
      "Train - top-10 acc: 0.59 loss: 4.66 \n",
      "Val   - top-10 acc: 0.13 loss: 11.14 \n",
      "Epoch 225\n",
      "Train - top-10 acc: 0.59 loss: 4.65 \n",
      "Val   - top-10 acc: 0.13 loss: 11.14 \n",
      "Epoch 226\n",
      "Train - top-10 acc: 0.59 loss: 4.65 \n",
      "Val   - top-10 acc: 0.13 loss: 11.14 \n",
      "Epoch 227\n",
      "Train - top-10 acc: 0.59 loss: 4.65 \n",
      "Val   - top-10 acc: 0.12 loss: 11.14 \n",
      "Epoch 228\n",
      "Train - top-10 acc: 0.59 loss: 4.64 \n",
      "Val   - top-10 acc: 0.12 loss: 11.15 \n",
      "Epoch 229\n",
      "Train - top-10 acc: 0.59 loss: 4.64 \n",
      "Val   - top-10 acc: 0.13 loss: 11.14 \n",
      "Epoch 230\n",
      "Train - top-10 acc: 0.59 loss: 4.63 \n",
      "Val   - top-10 acc: 0.13 loss: 11.15 \n",
      "Epoch 231\n",
      "Train - top-10 acc: 0.59 loss: 4.63 \n",
      "Val   - top-10 acc: 0.12 loss: 11.14 \n",
      "Epoch 232\n",
      "Train - top-10 acc: 0.59 loss: 4.63 \n",
      "Val   - top-10 acc: 0.12 loss: 11.14 \n",
      "Epoch 233\n",
      "Train - top-10 acc: 0.59 loss: 4.62 \n",
      "Val   - top-10 acc: 0.12 loss: 11.14 \n",
      "Epoch 234\n",
      "Train - top-10 acc: 0.59 loss: 4.62 \n",
      "Val   - top-10 acc: 0.12 loss: 11.15 \n",
      "Epoch 235\n",
      "Train - top-10 acc: 0.59 loss: 4.62 \n",
      "Val   - top-10 acc: 0.12 loss: 11.15 \n"
     ]
    }
   ],
   "source": [
    "opt = opt_(model.parameters(), lr)\n",
    "\n",
    "trainer = train_net(model, opt, loss_fn, val_metrics,\n",
    "        train_loader, val_loader, device)\n",
    "trainer.run(train_loader, max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, '../data/processed/train_dataset.pt')\n",
    "torch.save(val_dataset, '../data/processed/val_dataset.pt')\n",
    "torch.save(test_dataset, '../data/processed/test_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/train_dataset_00.pickle', 'rb') as f:\n",
    "    asdfasdf = pickle.load(f)\n",
    "asdfasdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#with open('../data/processed/train_dataset.pkl', 'wb') as f:\n",
    "#    pickle.dump(train_dataset, f)\n",
    "with open('../data/processed/val_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(val_dataset, f)\n",
    "#with open('../data/processed/test_dataset.pkl', 'wb') as f:\n",
    "#    pickle.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_d, val_d = train_test_split(train_d.items(), test_size=0.1, shuffle=True)\n",
    "train_d, val_d = dict(train_d), dict(val_d)\n",
    "train_dataset = YooChooseDataset(train_d)\n",
    "val_dataset = YooChooseDataset(val_d)\n",
    "test_dataset = YooChooseDataset(test_d)\n",
    "print('# train samples: {}\\n# val samples: {}\\n# test samples: {}'\n",
    "        .format(len(train_dataset), len(val_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서부터 subset을 배제하고 가야됨\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_dataset = YooChooseDataset(train_d)\n",
    "train_dataset, val_dataset = random_split(train_dataset,\n",
    "        [floor(0.9 * len(train_dataset)),\n",
    "                len(train_dataset) - floor(0.9 * len(train_dataset))])\n",
    "test_dataset = YooChooseDataset(test_d)\n",
    "print('# train samples: {}\\n# val samples: {}\\n# test samples: {}'\n",
    "        .format(len(train_dataset), len(val_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YooChooseDataset(Dataset):\n",
    "    def __init__(self, d):\n",
    "        super(YooChooseDataset, self).__init__()\n",
    "        self.samples = self.add_from_dict(d)\n",
    "        \n",
    "    def add_from_dict(self, d):\n",
    "        samples = []\n",
    "        for dd in d.values():\n",
    "            for data in dd:\n",
    "                samples.append(data)\n",
    "        return samples\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_ids, y = self.samples[idx]\n",
    "        x_ids_ = set(x_ids)\n",
    "        x_ids_.remove(x_ids[- 1])\n",
    "        x_ids_ = list(x_ids_) + [x_ids[- 1]]\n",
    "        x_dict = {x_id: i for i, x_id in enumerate(x_ids_)}\n",
    "        x = [[x_id] for x_id in x_ids_]\n",
    "        edge_dict = defaultdict(lambda: defaultdict(int)) # 얘네를 다 미리 저장해야 할 듯? f-b propagation step에 비해 오래 걸리는지 확인해보고 일단\n",
    "        for i in range(len(x_ids) - 1):\n",
    "            edge_dict[x_dict[x_ids[i]]][x_dict[x_ids[i + 1]]] += 1\n",
    "        edge_index, edge_weights = [], []\n",
    "        for o in edge_dict.keys():\n",
    "            s = sum(edge_dict[o].values())\n",
    "            for d in edge_dict[o].keys():\n",
    "                edge_index.append([o, d])\n",
    "                edge_weights.append(edge_dict[o][d] / s)\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_weights = torch.tensor(edge_weights)\n",
    "        return {\n",
    "                'graph': Data(x, edge_index=edge_index, edge_weights=edge_weights),\n",
    "                'label': y\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "remain_item = item_enc.transform(remain_item).tolist()\n",
    "item_enc = OneHotEncoder(sparse=False)\n",
    "item_enc.fit([[item] for item in remain_item])\n",
    "with open('../data/interim/onehotencoder.pkl', 'wb') as f:\n",
    "    pickle.dump(item_enc, f)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BaeJR_py36",
   "language": "python",
   "name": "eagle_baejr_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
