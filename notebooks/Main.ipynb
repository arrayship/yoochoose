{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from ignite.metrics import TopKCategoricalAccuracy, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 ** 13\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt_ = torch.optim.Adam\n",
    "lr = 0.00003\n",
    "val_metrics = {\n",
    "        'top-10 acc': TopKCategoricalAccuracy(10),\n",
    "        'loss': Loss(loss_fn)\n",
    "        }\n",
    "device = 'cuda:1'\n",
    "max_epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks = pd.read_csv('../data/raw/yoochoose-clicks.dat',\n",
    "        names=['sess', 'ts', 'item', 'cat'],  dtype={'cat': str},\n",
    "        usecols=['sess', 'ts', 'item'], header=None)\n",
    "clicks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt, timedelta as td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks['ts'] = clicks['ts'].apply(lambda s: dt.strptime(s[:19], '%Y-%m-%dT%H:%M:%S'))\n",
    "clicks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitdate = max(clicks['ts']) - td(1)\n",
    "item_count = clicks['item'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remain_sess = []\n",
    "remain_item = set()\n",
    "for _, group in clicks.groupby('sess', sort=False):\n",
    "    print(group.iat[0, 0], end='\\r')\n",
    "    gi = group['item'].tolist()\n",
    "    n = len(gi)\n",
    "    stop = False\n",
    "    if n > 1:\n",
    "        for item in gi:\n",
    "            if item_count[item] < 5:\n",
    "                stop = True\n",
    "                break\n",
    "    else:\n",
    "        stop = True\n",
    "    if not stop:\n",
    "        remain_sess.append((str(group.iat[0, 0]), group.iat[0, 1], gi))\n",
    "        for item in gi:\n",
    "            remain_item.add(item)\n",
    "with open('../data/interim/n_items.json', 'w') as f:\n",
    "    json.dump(len(remain_item), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "remain_item = list(remain_item)\n",
    "item_enc = LabelEncoder()\n",
    "item_enc.fit(remain_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = defaultdict(list)\n",
    "test_d = defaultdict(list)\n",
    "for sess, ts, items in remain_sess:\n",
    "    print(sess, end='\\r')\n",
    "    items = item_enc.transform(items).tolist()\n",
    "    if ts < splitdate:\n",
    "        for i in range(1, len(items)):\n",
    "            train_d[sess].append((items[: i], items[i]))\n",
    "    else:\n",
    "        for i in range(1, len(items)):\n",
    "            test_d[sess].append((items[: i], items[i]))\n",
    "with open('../data/interim/train.json', 'w') as f:\n",
    "    json.dump(train_d, f)\n",
    "with open('../data/interim/test.json', 'w') as f:\n",
    "    json.dump(test_d, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YooChooseDataset(Dataset):\n",
    "    def __init__(self, d):\n",
    "        super(YooChooseDataset, self).__init__()\n",
    "        self.samples = self.add_from_dict(d)\n",
    "        \n",
    "    def add_from_dict(self, d):\n",
    "        samples = []\n",
    "        for dd in d.values():\n",
    "            for data in dd:\n",
    "                samples.append(data)\n",
    "        \n",
    "        x_ids, y = self.samples[idx]\n",
    "        x_ids_ = set(x_ids)\n",
    "        x_ids_.remove(x_ids[- 1])\n",
    "        x_ids_ = list(x_ids_) + [x_ids[- 1]]\n",
    "        x_dict = {x_id: i for i, x_id in enumerate(x_ids_)}\n",
    "        x = [[x_id] for x_id in x_ids_]\n",
    "        edge_dict = defaultdict(lambda: defaultdict(int)) # 얘네를 다 미리 저장해야 할 듯? f-b propagation step에 비해 오래 걸리는지 확인해보고 일단\n",
    "        for i in range(len(x_ids) - 1):\n",
    "            edge_dict[x_dict[x_ids[i]]][x_dict[x_ids[i + 1]]] += 1\n",
    "        edge_index, edge_weights = [], []\n",
    "        for o in edge_dict.keys():\n",
    "            s = sum(edge_dict[o].values())\n",
    "            for d in edge_dict[o].keys():\n",
    "                edge_index.append([o, d])\n",
    "                edge_weights.append(edge_dict[o][d] / s)\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_weights = torch.tensor(edge_weights)\n",
    "        samples.append((Data(x, edge_index=edge_index, edge_weights=edge_weights), y))\n",
    "        return samples\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "                'graph': samples[idx][0],\n",
    "                'label': samples[idx][1]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/interim/n_items.json', 'r') as f:\n",
    "    n_items = json.load(f)\n",
    "with open('../data/interim/train.json', 'r') as f:\n",
    "    train_d = json.load(f)\n",
    "with open('../data/interim/test.json', 'r') as f:\n",
    "    test_d = json.load(f)\n",
    "print('# items: {}\\n# train sessions: {}\\n# test sessions: {}'\n",
    "        .format(n_items, len(train_d), len(test_d)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_dataset = YooChooseDataset(train_d)\n",
    "train_dataset, val_dataset = random_split(train_dataset,\n",
    "        [floor(0.9 * len(train_dataset)),\n",
    "                len(train_dataset) - floor(0.9 * len(train_dataset))])\n",
    "test_dataset = YooChooseDataset(test_d)\n",
    "print('# train samples: {}\\n# val samples: {}\\n# test samples: {}'\n",
    "        .format(len(train_dataset), len(val_dataset), len(test_dataset)))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=24)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GatedGraphConv\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.wq = nn.Linear(embed_dim, embed_dim)\n",
    "        self.wk = nn.Linear(embed_dim, embed_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.q = nn.Linear(embed_dim, 1)\n",
    "        self.w = nn.Linear(2 * embed_dim, embed_dim)\n",
    "        \n",
    "    def forward(self, x, batch):\n",
    "        sections = list(torch.bincount(batch).to('cpu').numpy())\n",
    "        x_split = torch.split(x, sections)\n",
    "        q_split = [x_[- 1].view(1, - 1) for x_ in x_split]\n",
    "        q = torch.cat([x_[- 1].view(1, - 1).repeat(x_.shape[0], 1) for x_ in x_split])\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(x)\n",
    "        a = self.q(q + k)\n",
    "        ax = a * x\n",
    "        ax_split = torch.split(ax, sections)\n",
    "        sg_split = [torch.sum(ax_, 0).view(1, - 1) for ax_ in ax_split]\n",
    "        sh_split = self.w(torch.cat((torch.cat(q_split), torch.cat(sg_split)), 1))\n",
    "        return sh_split\n",
    "    \n",
    "class PredProb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PredProb, self).__init__()\n",
    "    \n",
    "    def forward(self, sh, embedding):\n",
    "        return torch.mm(sh, embedding.weight.transpose(1, 0))\n",
    "\n",
    "class SRGNN(nn.Module):\n",
    "    def __init__(self, n_items, embed_dim):\n",
    "        super(SRGNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_items, embed_dim)\n",
    "        self.gatedgconv = GatedGraphConv(embed_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.attention = Attention(embed_dim)\n",
    "        self.predprob = PredProb()\n",
    "        \n",
    "    def _initialize_weights(self, ):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weights, batch =\\\n",
    "                data.x, data.edge_index, data.edge_weights, data.batch\n",
    "        x = self.embedding(x).squeeze()\n",
    "        x = self.gatedgconv(x, edge_index, edge_weights)\n",
    "        x = self.relu(x)\n",
    "        x = self.attention(x, batch)\n",
    "        x = self.predprob(x, self.embedding)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SRGNN(n_items, 128)\n",
    "for b in model.named_children():\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
    "\n",
    "def train_net(net, opt, loss_fn, val_metrics, train_loader, val_loader, device):\n",
    "    net.to(device)\n",
    "    def prepare_batch(batch, device, non_blocking=False):\n",
    "        x, y = batch.values()\n",
    "        return x.to(device), y.to(device)\n",
    "    def output_transform(x, y, y_pred, loss):\n",
    "        return (y_pred.max(1)[1], y)\n",
    "    trainer = create_supervised_trainer(net, opt, loss_fn, device,\n",
    "            prepare_batch=prepare_batch, output_transform=output_transform)\n",
    "    evaluator = create_supervised_evaluator(net, val_metrics, device,\n",
    "            prepare_batch=prepare_batch)\n",
    "    s = '{}: {:.2f} '\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_training_results(trainer):\n",
    "        evaluator.run(train_loader)\n",
    "        print('Epoch {}'.format(trainer.state.epoch))\n",
    "        message = 'Train - '\n",
    "        for k, v in val_metrics.keys():\n",
    "            message += s.format(m, evaluator.state.metrics[m])\n",
    "        print(message)\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def log_validation_results(trainer):\n",
    "        evaluator.run(val_loader)\n",
    "        message = 'Val   - '\n",
    "        for m in val_metrics.keys():\n",
    "            message += s.format(m, evaluator.state.metrics[m])\n",
    "        print(message)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = opt_(model.parameters(), lr)\n",
    "\n",
    "trainer = train_net(model, opt, loss_fn, val_metrics,\n",
    "        train_loader, val_loader, device)\n",
    "trainer.run(train_loader, max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeee = nn.Embedding(n_items, 8)\n",
    "gggg = GatedGraphConv(8, 1)\n",
    "ee2ss = Embedding2Score(8)\n",
    "xxxx = gggg(eeee(test_dataset[46].x).squeeze(), test_dataset[46].edge_index)\n",
    "xxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee2ss(xxxx, eeee, batch=torch.tensor([0, 0, 0, 0, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 50\n",
    "print(test_dataset[idx].x)\n",
    "print(test_dataset[idx].edge_index)\n",
    "print(test_dataset[idx].edge_weights)\n",
    "print(test_dataset[idx].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YooChooseDataset(Dataset):\n",
    "    def __init__(self, d):\n",
    "        super(YooChooseDataset, self).__init__()\n",
    "        self.samples = self.add_from_dict(d)\n",
    "        \n",
    "    def add_from_dict(self, d):\n",
    "        samples = []\n",
    "        for dd in d.values():\n",
    "            for data in dd:\n",
    "                samples.append(data)\n",
    "        return samples\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_ids, y = self.samples[idx]\n",
    "        x_ids_ = set(x_ids)\n",
    "        x_ids_.remove(x_ids[- 1])\n",
    "        x_ids_ = list(x_ids_) + [x_ids[- 1]]\n",
    "        x_dict = {x_id: i for i, x_id in enumerate(x_ids_)}\n",
    "        x = [[x_id] for x_id in x_ids_]\n",
    "        edge_dict = defaultdict(lambda: defaultdict(int)) # 얘네를 다 미리 저장해야 할 듯? f-b propagation step에 비해 오래 걸리는지 확인해보고 일단\n",
    "        for i in range(len(x_ids) - 1):\n",
    "            edge_dict[x_dict[x_ids[i]]][x_dict[x_ids[i + 1]]] += 1\n",
    "        edge_index, edge_weights = [], []\n",
    "        for o in edge_dict.keys():\n",
    "            s = sum(edge_dict[o].values())\n",
    "            for d in edge_dict[o].keys():\n",
    "                edge_index.append([o, d])\n",
    "                edge_weights.append(edge_dict[o][d] / s)\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_weights = torch.tensor(edge_weights)\n",
    "        return {\n",
    "                'graph': Data(x, edge_index=edge_index, edge_weights=edge_weights),\n",
    "                'label': y\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "remain_item = item_enc.transform(remain_item).tolist()\n",
    "item_enc = OneHotEncoder(sparse=False)\n",
    "item_enc.fit([[item] for item in remain_item])\n",
    "with open('../data/interim/onehotencoder.pkl', 'wb') as f:\n",
    "    pickle.dump(item_enc, f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('../data/raw/yoochoose-clicks.dat', 'r') as f:\n",
    "    reader = csv.DictReader(f, delimiter=',')\n",
    "    for i, data in enumerate(reader):\n",
    "        print(data)\n",
    "        if i == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buys.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BaeJR_py36",
   "language": "python",
   "name": "eagle_baejr_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
